{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NNFS Chapter 1 - Introducing Neural Networks\n",
    "\n",
    "* [Visualizing Neural Network Sizes](https://nnfs.io/ntr)\n",
    "* [How weights and biases impact a single neuron](https://nnfs.io/bru/)\n",
    "* [Visualization of an example Dogs vs Cats neural network classifier](https://nnfs.io/qtb/)\n",
    "* [Visualization of the forward pass calculation and path for a neural network](https://nnfs.io/vkr/)\n",
    "\n",
    "For a step function, if the neuron’s output value, which is calculated by `sum(inputs · weights) + bias`, is greater than 0, the neuron fires (so it would output a 1).\n",
    "\n",
    "Otherwise, it does not fire and would pass along a 0. The formula for a single neuron might look something like:\n",
    "\n",
    "```\n",
    "output = sum(inputs * weights) + bias\n",
    "````\n",
    "\n",
    "The thing that makes neural networks appear challenging, is the math that is involved, and how scary it can sometimes look. For example, let’s imagine a neural network, and take a journey through what’s going on during a simple forward pass of data, and the math behind it. For this, do not worry about understanding every- thing. The idea here is to give you a high level idea of what’s going on overall, and then the point of this book is to break down each of these elements into painfully simple explanations, which will cover both forward and backward passes involved in training neural networks.\n",
    "\n",
    "A typical neural network has thousands or even up to millions of adjustable parameters (weights and biases). In this way, neural networks act as enormous functions with vast numbers of parameters. The concept of a long function with millions of variables that could be used to solve a problem isn’t all too difficult. With that many variables related to neurons, arranged as interconnected layers, we can imagine there exist some combinations of values for these variables that will yield desired outputs. Finding that combination of parameter (weight and bias) values is the challenging part.\n",
    "The end goal for neural networks is to adjust their weights and biases (the parameters), so when applied to a yet-unseen example in the input, they produce the desired output. When supervised machine learning algorithms are trained, we show the algorithm examples of inputs and their associated desired outputs. One major issue with this concept is overfitting — when the algorithm only learns to fit the training data but doesn’t actually “understand” anything about underlying input-output dependencies. The network basically just “memorizes” the training data.\n",
    "\n",
    "Thus, we tend to use “in-sample” data to train a model and then use “out-of-sample” data to validate an algorithm (or a neural network model in our case). Certain percentages are set aside for both datasets to partition the data. For example, if there is a dataset of 100,000 samples of data and labels, you will immediately take 10,000 and set them aside to be your “out-of-sample” or “validation” data. You will then train your model with the other 90,000 in-sample or “training” data and finally validate your model with the 10,000 out-of-sample data that the model hasn’t yet seen. The goal is for the model to not only accurately predict on the training data, but also to be similarly accurate while predicting on the withheld out-of-sample validation data.\n",
    "This is called generalization, which means learning to fit the data instead of memorizing it. The idea is that we “train” (slowly adjusting weights and biases) a neural network on many examples of data. We then take out-of-sample data that the neural network has never been presented with and hope it can accurately predict on these data too.\n",
    "\n",
    "You should now have a general understanding of what neural networks are, or at least what the objective is, and how we plan to meet this objective. To train these neural networks, we calculate  how “wrong” they are using algorithms to calculate the error (called loss), and attempt to slowly adjust their parameters (weights and biases) so that, over many iterations, the network gradually becomes less wrong. The goal of all neural networks is to generalize, meaning the network can see many examples of never-before-seen data, and accurately output the values we hope to achieve. Neural networks can be used for more than just classification. They can perform regression (predict a scalar, singular, value), clustering (assign unstructured data into groups), and many other tasks. Classification is just a common task for neural networks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "2.8"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_output(inputs, weights, bias):\n",
    "    output = (\n",
    "            inputs[0] * weights[0] +\n",
    "            inputs[1] * weights[1] +\n",
    "            inputs[2] * weights[2] + bias\n",
    "    )\n",
    "    return output\n",
    "\n",
    "compute_output(\n",
    "    inputs=[0, 1, 2],\n",
    "    weights=[0.1, 0.2, 0.3],\n",
    "    bias=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}