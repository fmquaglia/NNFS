{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 5 - Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categorical Cross Entropy\n",
    "\n",
    "$$\n",
    "L_i = -\\sum_j y_{i,j}log(\\hat{y}_{i,j})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "  * $L_i$ Sample loss value\n",
    "  * $i$ ith sample\n",
    "  * $j$ is the label/output index\n",
    "  * $y$ target value\n",
    "  * $\\hat{y}$ predicted values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# data\n",
    "softmax_output = [0.7, 0.1, 0.2] # y hat\n",
    "target_output = [1, 0, 0] # y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# calculate loss function\n",
    "loss = -(\n",
    "    math.log(softmax_output[0]) * target_output[0] +\n",
    "    math.log(softmax_output[1]) * target_output[1] +\n",
    "    math.log(softmax_output[2]) * target_output[2]\n",
    ")\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since `target_output[1]` and `target_output[2]` are 0 we can omit these terms.\n",
    "And given that `target_output[0]` is 1, we can simplify the above as:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2] # y hat\n",
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, array([0.7, 0.1, 0.2])), (1, array([0.1, 0.5, 0.4])), (1, array([0.02, 0.9 , 0.08]))]\n",
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# Probabilities for 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]  # dog, cat, cat\n",
    "\n",
    "indexed_distributions = zip(class_targets, softmax_outputs)\n",
    "print(list(indexed_distributions))\n",
    "\n",
    "for target_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[target_idx])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    softmax_outputs[\n",
    "        range(len(softmax_outputs)), # first dimension, list of numbers from 0 to last index. We have as many indices as distributions, so we can use a range()\n",
    "        class_targets\n",
    "    ]\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    -np.log(\n",
    "        softmax_outputs[\n",
    "            range(len(softmax_outputs)),\n",
    "            class_targets\n",
    "        ]\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# we want an average loss per batch to have an idea about how our model is doing during training\n",
    "neg_log = -np.log(\n",
    "    softmax_outputs[\n",
    "        range(len(softmax_outputs)), class_targets\n",
    "    ]\n",
    ")\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Targets can be:\n",
    "  * **one-hot encoded**: all values, except for one, are zeros and the correct label’s position is filled with 1)\n",
    "  * **sparse**: the numbers they contain are the correct class numbers\n",
    "\n",
    "We have to add a check if they are one-hot encoded and handle it differently if that's case.\n",
    "\n",
    "The check can be performed by counting the dimensions:\n",
    "  * if targets are single-dimensional (like a list), they are sparse\n",
    "  * if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors.\n",
    "\n",
    " In this second case, we’ll implement a solution using the first equation from this chapter, instead of filtering out the confidences at the target labels.\n",
    "\n",
    "We have to multiply confidences by the targets, zeroing out all values except the ones at correct labels, performing a sum along the row axis (axis 1).\n",
    "We have to add a test to the code we just wrote for the number of dimensions, move calculations of the log values outside this new if statement, and implement the solution for the one-hot encoded labels following the first equation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "\n",
    "# Probabilities for target values\n",
    "if len(class_targets.shape) == 1: # only if categorical labels\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "elif len(class_targets.shape) == 2: # only for one-hot encoded labels\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs * class_targets,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "neg_log = -np.log(correct_confidences)\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\lim_{x\\rightarrow 0} log(x) = -\\infty\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabricio/opt/anaconda3/envs/NNFS/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": "inf"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0) # RuntimeWarning: divide by zero encountered in log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.e**(-np.inf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-07\n",
      "0.9999999\n"
     ]
    }
   ],
   "source": [
    "print(np.clip(0, 1e-7, 1 - 1e-7))\n",
    "print(np.clip(1, 1e-7, 1 - 1e-7))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categorical Cross-Entropy Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combining all together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104\n",
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second activation function and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Final code for chapter](https://github.com/Sentdex/nnfs_book/tree/main/Chapter_5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
