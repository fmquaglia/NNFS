{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 4 - Activation Functions"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU Activation Function Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "outputs = []\n",
    "\n",
    "for i in inputs:\n",
    "    outputs.append(i if i > 0 else 0)\n",
    "\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "outputs = []\n",
    "\n",
    "for i in inputs:\n",
    "    outputs.append(max(0, i))\n",
    "\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0. , 2. , 0. , 3.3, 0. , 1.1, 2.2, 0. ])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "outputs = np.maximum(0, inputs)\n",
    "\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        ],\n       [0.        , 0.00011395, 0.        ],\n       [0.        , 0.00031729, 0.        ],\n       [0.        , 0.00052666, 0.        ],\n       [0.        , 0.00071401, 0.        ]], dtype=float32)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nnfs\n",
    "nnfs.init() #We will also use this package to ensure repeatability for everyone, using nnfs.init(), after importing NumPy\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "activation1.output[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax activation function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values: [121.51041751893969, 3.3534846525504487, 10.85906266492961]\n",
      "sum of exponentiated values: 135.72296483641975\n",
      "normalized exponentiated values: [0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
      "sum of the expoentiated values (should be 1): 1.0\n",
      "Normalized exponentiated values:\n",
      "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
      "Sum of normalized values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Values from the previous output when we described\n",
    "# what a neural network is\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# e - mathematical constant, we use E here to match a common coding\n",
    "# style where constants are uppercased\n",
    "E = 2.71828182846  # you can also use math.e\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output)  # ** - power operator in Python\n",
    "\n",
    "# this is how I implemented it on my own\n",
    "summed_exp = sum(exp_values)\n",
    "normalized_exp = list(map(lambda x: x/summed_exp, exp_values))\n",
    "normalized_exp_sum = sum(normalized_exp)\n",
    "print('exponentiated values: {}'.format(exp_values))\n",
    "print('sum of exponentiated values: {}'.format(summed_exp))\n",
    "print('normalized exponentiated values: {}'.format(normalized_exp))\n",
    "print('sum of the expoentiated values (should be 1): {}'.format(normalized_exp_sum))\n",
    "\n",
    "# here's the book's solution\n",
    "# Now normalize values\n",
    "norm_base = sum(exp_values)  # We sum all values\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('Sum of normalized values:', sum(norm_values))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use numpy to do the same thing in a more compact way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Values from the earlier previous when we described\n",
    "# what a neural network is\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We’re trying to sum all the outputs from a layer for each sample in a batch; converting the layer’s output array with row length equal to the number of neurons in the layer, to just one value.\n",
    "We need a column vector with these values since it will let us normalize the whole batch of samples, sample-wise, with a single calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis 1, but keep the same dimensions as input:\n",
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis 1, but keep the same dimensions as input:')\n",
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                          [8.9, -1.81, 0.2],\n",
    "                          [1.41, 1.051, 0.026]])\n",
    "print(np.sum(layer_outputs, axis=1, keepdims=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two main challenges with neural networks:\n",
    "\n",
    "- “dead neurons”\n",
    "- very large numbers (referred to as “exploding” values).\n",
    "\n",
    "The exponential function used in softmax activation is one of the sources of exploding values.\n",
    "With Softmax, thanks to the normalization, we can subtract any value from all the inputs, and it will not change the output:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n",
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "softmax.forward([[1, 2, 3]])\n",
    "print(softmax.output)\n",
    "softmax.forward([[-2, -1, 0]])  # subtracted 3 - max from the list\n",
    "print(softmax.output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can add another dense layer as the output layer, setting it to contain as many inputs as the previous layer has outputs and as many outputs as our data includes classes. Then we can apply the softmax activation to the output of this new layer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the distribution of predictions is almost equal, as each of the samples has ~33% (0.33) predictions for each class. This results from the random initialization of weights (a draw from the normal distribution, as not every random initialization will result in this) and zeroed biases.\n",
    "\n",
    "These outputs are also our “confidence scores.” To determine which classification the model has chosen to be the prediction, we perform an *argmax* on these outputs, which checks which of the classes in the output distribution has the highest confidence and returns its index - the predicted class index.\n",
    "\n",
    "That said, the confidence score can be as important as the class prediction itself. For example, the argmax of [0.22, 0.6, 0.18] **is the same as the argmax for [0.32, 0.36, 0.32]. In both of these, the argmax function would return an index value of 1 (the 2nd element in Python’s zero-indexed paradigm), but obviously, a 60% confidence is much better than a 36% confidence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
